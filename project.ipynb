{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ForestPearson/CS410-510-NLP-project/blob/lstm-replacement/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "suR1yK_MgUwt",
        "outputId": "a3619df4-bd99-4faa-bc90-2cb471ffc522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/ForestPearson/CS410-510-NLP-project/main/data/combined.txt\n",
            "389861/389861 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EPOCHS = 30\n",
        "DIM = 256\n",
        "RNN = 1024\n",
        "\n",
        "path = tf.keras.utils.get_file('combined.txt', 'https://raw.githubusercontent.com/ForestPearson/CS410-510-NLP-project/main/data/combined.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_gdgLYhjp0",
        "outputId": "4e112475-76a7-4b89-eab3-87095549bb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 389861\n",
            "ACT I\n",
            "\n",
            "SCENE I. Rousillon. The COUNT's palace.\n",
            "\n",
            "Enter BERTRAM, the COUNTESS of Rousillon, HELENA, and LAFEU, all in black\n",
            "COUNTESS\n",
            "In delivering my son from me, I bury a second husband.\n",
            "BERTRAM\n",
            "And I in going, madam, weep o'er my father's death\n",
            "anew: but I must attend his majesty's command, to\n",
            "whom I am now in ward, evermore in subjection.\n",
            "LAFEU\n",
            "You shall find of the king a husband, madam; you,\n",
            "sir, a father: he that so generally is at all times\n",
            "good must of necessity hold his virtue to you; who\n"
          ]
        }
      ],
      "source": [
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "print(\"Length:\", len(text))\n",
        "print(text[:500])\n",
        "\n",
        "vocab = sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "50_P7GzQjlT1"
      },
      "outputs": [],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "ids_from_chars = StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "vocabSize = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "chars = chars_from_ids(ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EZRNAQSwn2dn"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "orOMI3FImLsQ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "#Convert to character indices\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "#Form sequences made up of 100 characters\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IRXDSYnnoCJN",
        "outputId": "1ea64cce-06b2-4b87-82d8-ae5aef5ae4a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Training data creation and target creation using sequences\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tkcjqI8Opmyx"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = LSTM(rnn_units,return_sequences=True,return_state=True)\n",
        "    self.dense = Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    # x, states = self.lstm(x, initial_state=states, training=training)\n",
        "    x, memory_state, carry_state = self.lstm(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      # return x, states\n",
        "      return x, memory_state, carry_state\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qGvpyp38phUW"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocabSize,\n",
        "    embedding_dim=DIM,\n",
        "    rnn_units=RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YNGsrLBFqsng"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "# Directory where the checkpoints will be saved\n",
        "dir = './data/epochs'\n",
        "#File names\n",
        "fileName = os.path.join(dir, \"ckpt_{epoch}\")\n",
        "results = tf.keras.callbacks.ModelCheckpoint(filepath=fileName,save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMvVffRCrB_5",
        "outputId": "e617475a-c683-4e53-f974-fb169119c76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "60/60 [==============================] - 7s 65ms/step - loss: 3.3690\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - 5s 69ms/step - loss: 2.6806\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - 5s 68ms/step - loss: 2.3456\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - 5s 69ms/step - loss: 2.1914\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - 5s 67ms/step - loss: 2.0527\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 1.9346\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.8349\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - 5s 69ms/step - loss: 1.7532\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - 5s 69ms/step - loss: 1.6821\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - 5s 68ms/step - loss: 1.6255\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.5740\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.5293\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.4889\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 1.4533\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.4202\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - 5s 68ms/step - loss: 1.3892\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - 5s 68ms/step - loss: 1.3589\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 1.3298\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 1.2993\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - 5s 68ms/step - loss: 1.2715\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - 4s 66ms/step - loss: 1.2417\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 1.2110\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 1.1791\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.1440\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.1098\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - 5s 69ms/step - loss: 1.0752\n",
            "Epoch 27/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 1.0340\n",
            "Epoch 28/30\n",
            "60/60 [==============================] - 4s 67ms/step - loss: 0.9930\n",
            "Epoch 29/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 0.9490\n",
            "Epoch 30/30\n",
            "60/60 [==============================] - 5s 68ms/step - loss: 0.9015\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[results])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VndZinTK3Hku"
      },
      "outputs": [],
      "source": [
        "class Generate(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  def predict(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # predicted_logits, states = self.model(inputs=input_ids, states=states,return_state=True)\n",
        "    predicted_logits, memory_state, carry_state = self.model(inputs=input_ids, states=states,return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # return predicted_chars, states\n",
        "    return predicted_chars, memory_state, carry_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aCzaqNFq3Hkv",
        "outputId": "b1f515ad-c6e8-409b-d801-6b6d8dc8251a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COUNTESS\n",
            "\n",
            "Figey ayo mat.\n",
            "\n",
            "PSin if otle hey I then d: n:\n",
            "PANDZime\n",
            "S\n",
            "G-] t LINAn had cos\n",
            "he ives t bry t\n",
            "perer-tllllothid bI cus canon iousamo whas.\n",
            "Hele\n",
            "G\n",
            "AMitheverducalonseveath chive\n",
            "I't w nd d\n",
            "Win nce u o the wie mixe, chemoungayo'sivecke. I'seate atha yoshts golor:\n",
            "S\n",
            "Congala'ttong thyithe icust f tawistoughe myongomaves this!\n",
            "M[Shainke\n",
            "PS\n",
            "ELYCELLI'llear buprms\n",
            "S\n",
            "SAThin\n",
            "LOShafawithe readyok om ar ourdmau cheacares ley f I'ld anoupawertr hat Thine ar h t.\n",
            "payoranken ie, tathed toow nsse ayonite t bie ayoumoufoun cowx the has s cth yorkyomes edifere ber thengal gr empit,\n",
            "HI wewin ate\n",
            "Sh cu d fapre, t d sen fl,\n",
            "I an omaundgato falll thedsspan, soo ome\n",
            "Th I cis\n",
            "O'd tomo\n",
            "HS\n",
            "PAUKI'teatond JANUTr, n imeele mange w.\n",
            "aremprd\n",
            "HES\n",
            "I't whostr: oud mard hered 'dyonf.\n",
            "Whaly owns.\n",
            "Shid farond oumoruso'LERomawice\n",
            "Cawit, alaind\n",
            "DINELours INKn-pearat fazeeayoure stur an test msh he modou I y momy, ssosiowhal ghainfere.\n",
            "S\n",
            "IUYUS\n",
            "EShe sertay me il dechenf bee bLighinge dedemacrco s po'shteead me tos incovemelwea \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Generator = Generate(model, chars_from_ids, ids_from_chars)\n",
        "states = None\n",
        "seed = tf.constant(['COUNTESS'])\n",
        "result = [seed]\n",
        "\n",
        "for n in range(1000):\n",
        "  # seed, states = Generator.predict(seed, states=states)\n",
        "  seed, memory_state, carry_state = Generator.predict(seed, states=states)\n",
        "  result.append(seed)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "866cd1245b3823e185398c1063f44bf9b1cf162f43ea0435a9319e64a7104072"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}