{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ForestPearson/CS410-510-NLP-project/blob/lstm/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suR1yK_MgUwt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers import StringLookup\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import random\n",
        "import sys\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EPOCHS = 30\n",
        "DIM = 256\n",
        "RNN = 126\n",
        "\n",
        "path = tf.keras.utils.get_file('combined.txt', 'https://raw.githubusercontent.com/ForestPearson/CS410-510-NLP-project/main/data/combined.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_gdgLYhjp0",
        "outputId": "2fba8c84-f1d6-4c51-f7d8-653b9c3ae5f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length: 389861\n",
            "ACT I\n",
            "\n",
            "SCENE I. Rousillon. The COUNT's palace.\n",
            "\n",
            "Enter BERTRAM, the COUNTESS of Rousillon, HELENA, and LAFEU, all in black\n",
            "COUNTESS\n",
            "In delivering my son from me, I bury a second husband.\n",
            "BERTRAM\n",
            "And I in going, madam, weep o'er my father's death\n",
            "anew: but I must attend his majesty's command, to\n",
            "whom I am now in ward, evermore in subjection.\n",
            "LAFEU\n",
            "You shall find of the king a husband, madam; you,\n",
            "sir, a father: he that so generally is at all times\n",
            "good must of necessity hold his virtue to you; who\n"
          ]
        }
      ],
      "source": [
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "print(\"Length:\", len(text))\n",
        "print(text[:500])\n",
        "\n",
        "vocab = sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HkfYI5xwinj",
        "outputId": "43157cfe-3017-45fd-f6eb-9bde5a6b1d7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '&', \"'\", ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "ids_from_charsT = StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_idsT = StringLookup(vocabulary=ids_from_charsT.get_vocabulary(), invert=True, mask_token=None)\n",
        "chars_from_ids = dict((c, i) for i, c in enumerate(vocab))\n",
        "ids_from_chars = dict((i, c) for i, c in enumerate(vocab))\n",
        "  \n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSUV3j_4winl"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "steps = 5\n",
        "sequences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - seq_length, steps):\n",
        "    sequences.append(text[i: i + seq_length])\n",
        "    next_chars.append(text[i + seq_length])\n",
        "\n",
        "X = np.zeros((len(sequences), seq_length, len(vocab)), dtype = bool)\n",
        "y = np.zeros((len(sequences), len(vocab)), dtype = bool)\n",
        "for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        X[i, t, chars_from_ids[char]] = 1\n",
        "    y[i, chars_from_ids[next_chars[i]]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91voMiFGwinl",
        "outputId": "abe4c3df-ad8a-42f0-ef4c-0ecddbf782c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_4 (LSTM)               (None, 126)               96768     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 65)                8255      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 65)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 105,023\n",
            "Trainable params: 105,023\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "#model.add(GRU(128, input_shape =(seq_length, len(vocab))))\n",
        "model.add(LSTM(RNN, input_shape =(seq_length, len(vocab))))\n",
        "model.add(Dense(len(vocab)))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "#model.compile(loss ='categorical_crossentropy', optimizer = RMSprop(learning_rate= 0.01))\n",
        "model.compile(loss ='categorical_crossentropy', optimizer = 'adam')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhzCY5Ifwinm"
      },
      "outputs": [],
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "    print('\\nEpoch:',epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - seq_length - 1)\n",
        "  \n",
        "    for temperature in [0.5]:\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + seq_length]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "  \n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, seq_length, len(vocab)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, chars_from_ids[char]] = 1.\n",
        "                \n",
        "            preds = model.predict(x_pred, verbose = 0)[0]\n",
        "            preds = np.asarray(preds).astype('float64')\n",
        "            preds = np.log(preds) / temperature\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            next_index = np.argmax(np.random.multinomial(1, preds, 1))\n",
        "            next_char = ids_from_chars[next_index]\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "  \n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ppj7h02winn"
      },
      "outputs": [],
      "source": [
        "dir = '.\\data\\epochs.hdf5'\n",
        "checkpoint = ModelCheckpoint(dir, monitor ='loss',verbose = 1, save_best_only = True,mode ='min')\n",
        "reduce_alpha = ReduceLROnPlateau(monitor ='loss', factor = 0.2,patience = 1, min_lr = 0.001)\n",
        "#callbacks = [print_callback, checkpoint, reduce_alpha]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvmv9Lqgwinn",
        "outputId": "1446bbc2-d518-4dff-f49b-cc81d0764b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1213/1219 [============================>.] - ETA: 0s - loss: 2.8060\n",
            "Epoch 1: loss improved from inf to 2.80427, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 10s 7ms/step - loss: 2.8043 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 2.3326\n",
            "Epoch 2: loss improved from 2.80427 to 2.33262, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 2.3326 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "1216/1219 [============================>.] - ETA: 0s - loss: 2.2017\n",
            "Epoch 3: loss improved from 2.33262 to 2.20147, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 2.2015 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "1216/1219 [============================>.] - ETA: 0s - loss: 2.1131\n",
            "Epoch 4: loss improved from 2.20147 to 2.11315, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 6ms/step - loss: 2.1131 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 2.0399\n",
            "Epoch 5: loss improved from 2.11315 to 2.03989, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 7s 6ms/step - loss: 2.0399 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.9791\n",
            "Epoch 6: loss improved from 2.03989 to 1.97906, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 6ms/step - loss: 1.9791 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "1217/1219 [============================>.] - ETA: 0s - loss: 1.9267\n",
            "Epoch 7: loss improved from 1.97906 to 1.92680, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 6ms/step - loss: 1.9268 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "1214/1219 [============================>.] - ETA: 0s - loss: 1.8822\n",
            "Epoch 8: loss improved from 1.92680 to 1.88201, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.8820 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "1217/1219 [============================>.] - ETA: 0s - loss: 1.8584\n",
            "Epoch 9: loss improved from 1.88201 to 1.85844, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 9s 7ms/step - loss: 1.8584 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "1217/1219 [============================>.] - ETA: 0s - loss: 1.8085\n",
            "Epoch 10: loss improved from 1.85844 to 1.80864, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.8086 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 1.7757\n",
            "Epoch 11: loss improved from 1.80864 to 1.77573, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.7757 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "1215/1219 [============================>.] - ETA: 0s - loss: 1.7456\n",
            "Epoch 12: loss improved from 1.77573 to 1.74561, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.7456 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "1213/1219 [============================>.] - ETA: 0s - loss: 1.7227\n",
            "Epoch 13: loss improved from 1.74561 to 1.72218, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.7222 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "1216/1219 [============================>.] - ETA: 0s - loss: 1.6929\n",
            "Epoch 14: loss improved from 1.72218 to 1.69265, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.6927 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "1216/1219 [============================>.] - ETA: 0s - loss: 1.6695\n",
            "Epoch 15: loss improved from 1.69265 to 1.66937, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.6694 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "1217/1219 [============================>.] - ETA: 0s - loss: 1.6466\n",
            "Epoch 16: loss improved from 1.66937 to 1.64666, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.6467 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "1214/1219 [============================>.] - ETA: 0s - loss: 1.6254\n",
            "Epoch 17: loss improved from 1.64666 to 1.62538, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.6254 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.6072\n",
            "Epoch 18: loss improved from 1.62538 to 1.60719, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.6072 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "1213/1219 [============================>.] - ETA: 0s - loss: 1.5856\n",
            "Epoch 19: loss improved from 1.60719 to 1.58546, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.5855 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "1217/1219 [============================>.] - ETA: 0s - loss: 1.5700\n",
            "Epoch 20: loss improved from 1.58546 to 1.56969, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.5697 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.5518\n",
            "Epoch 21: loss improved from 1.56969 to 1.55186, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.5519 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.5347\n",
            "Epoch 22: loss improved from 1.55186 to 1.53473, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.5347 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.5188\n",
            "Epoch 23: loss improved from 1.53473 to 1.51878, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.5188 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "1216/1219 [============================>.] - ETA: 0s - loss: 1.5011\n",
            "Epoch 24: loss improved from 1.51878 to 1.50124, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.5012 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "1215/1219 [============================>.] - ETA: 0s - loss: 1.4864\n",
            "Epoch 25: loss improved from 1.50124 to 1.48654, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.4865 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "1213/1219 [============================>.] - ETA: 0s - loss: 1.4756\n",
            "Epoch 26: loss improved from 1.48654 to 1.47526, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.4753 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.4583\n",
            "Epoch 27: loss improved from 1.47526 to 1.45828, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.4583 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.4419\n",
            "Epoch 28: loss improved from 1.45828 to 1.44195, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.4420 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.4307\n",
            "Epoch 29: loss improved from 1.44195 to 1.43069, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.4307 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "1217/1219 [============================>.] - ETA: 0s - loss: 1.4168\n",
            "Epoch 30: loss improved from 1.43069 to 1.41693, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.4169 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "1216/1219 [============================>.] - ETA: 0s - loss: 1.4025\n",
            "Epoch 31: loss improved from 1.41693 to 1.40250, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.4025 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.3953\n",
            "Epoch 32: loss improved from 1.40250 to 1.39533, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3953 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "1217/1219 [============================>.] - ETA: 0s - loss: 1.3765\n",
            "Epoch 33: loss improved from 1.39533 to 1.37656, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3766 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "1212/1219 [============================>.] - ETA: 0s - loss: 1.3632\n",
            "Epoch 34: loss improved from 1.37656 to 1.36347, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3635 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "1214/1219 [============================>.] - ETA: 0s - loss: 1.3522\n",
            "Epoch 35: loss improved from 1.36347 to 1.35186, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3519 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 1.3408\n",
            "Epoch 36: loss improved from 1.35186 to 1.34084, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3408 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "1216/1219 [============================>.] - ETA: 0s - loss: 1.3280\n",
            "Epoch 37: loss improved from 1.34084 to 1.32797, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3280 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "1215/1219 [============================>.] - ETA: 0s - loss: 1.3161\n",
            "Epoch 38: loss improved from 1.32797 to 1.31604, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3160 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "1211/1219 [============================>.] - ETA: 0s - loss: 1.3039\n",
            "Epoch 39: loss improved from 1.31604 to 1.30374, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.3037 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "1213/1219 [============================>.] - ETA: 0s - loss: 1.2916\n",
            "Epoch 40: loss improved from 1.30374 to 1.29170, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2917 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 1.2809\n",
            "Epoch 41: loss improved from 1.29170 to 1.28092, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2809 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "1215/1219 [============================>.] - ETA: 0s - loss: 1.2685\n",
            "Epoch 42: loss improved from 1.28092 to 1.26859, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2686 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "1214/1219 [============================>.] - ETA: 0s - loss: 1.2583\n",
            "Epoch 43: loss improved from 1.26859 to 1.25870, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2587 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "1215/1219 [============================>.] - ETA: 0s - loss: 1.2470\n",
            "Epoch 44: loss improved from 1.25870 to 1.24700, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2470 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "1215/1219 [============================>.] - ETA: 0s - loss: 1.2456\n",
            "Epoch 45: loss improved from 1.24700 to 1.24554, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2455 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "1214/1219 [============================>.] - ETA: 0s - loss: 1.2240\n",
            "Epoch 46: loss improved from 1.24554 to 1.22418, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2242 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "1214/1219 [============================>.] - ETA: 0s - loss: 1.2182\n",
            "Epoch 47: loss improved from 1.22418 to 1.21812, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2181 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "1218/1219 [============================>.] - ETA: 0s - loss: 1.2055\n",
            "Epoch 48: loss improved from 1.21812 to 1.20547, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.2055 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "1219/1219 [==============================] - ETA: 0s - loss: 1.1987\n",
            "Epoch 49: loss improved from 1.20547 to 1.19867, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.1987 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "1213/1219 [============================>.] - ETA: 0s - loss: 1.1873\n",
            "Epoch 50: loss improved from 1.19867 to 1.18754, saving model to .\\data\\epochs.hdf5\n",
            "1219/1219 [==============================] - 8s 7ms/step - loss: 1.1875 - lr: 0.0010\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1f8e307d240>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model.fit(X, y, BATCH_SIZE, epochs = 30, callbacks = callbacks)\n",
        "\n",
        "model.fit(X, y, BATCH_SIZE, epochs = 50, callbacks=[checkpoint, reduce_alpha])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_nCYyh0wino",
        "outputId": "e78c298e-d880-4189-8e26-efeba542a33e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e your sorrow and my grief\n",
            "Were both extermined.\n",
            "PHEBE\n",
            "Thou hast my love: is not that neighbourly?\n",
            "Sfeazt and Alle d and are to could twose\n",
            "her name you, good bemaress down frield, bring scort.\n",
            "Whou, but my hands one lack: and to read bed,\n",
            "Beforaster.\n",
            "Exeunt\n",
            " hall not a cruef for do, on, our greping\n",
            "And woman in the ropes offen of more as mickiness for theel\n",
            "Would: be parsuge, noted as lyol; hark fortun\n",
            "Becaouce her to tit your lovens and love\n",
            "I doer goad and paless. Ham.\n",
            "HER\n",
            "Why, day you, of her never for Char:\n",
            "All Grach high deme: befores and all the fails;\n",
            "FLORIZEL\n",
            "By coulle. I have my rife\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def generate(length, temperature):\n",
        "    #Get the randomly selecting starting sequence\n",
        "    seed = random.randint(0, len(text) - seq_length - 1)\n",
        "    generated = ''\n",
        "    sentence = text[seed: seed + seq_length]\n",
        "    generated += sentence\n",
        "    #Predict and apped text based upon the seed\n",
        "    for i in range(length):\n",
        "            x_pred = np.zeros((1, seq_length, len(vocab)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, chars_from_ids[char]] = 1.\n",
        "            preds = model.predict(x_pred, verbose = 0)[0]\n",
        "            \n",
        "            preds = np.asarray(preds).astype('float64')\n",
        "            preds = np.log(preds) / temperature\n",
        "            exp_preds = np.exp(preds)\n",
        "            preds = exp_preds / np.sum(exp_preds)\n",
        "            next_index = np.argmax(np.random.multinomial(1, preds, 1))\n",
        "            \n",
        "            next_char = ids_from_chars[next_index]\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "    return generated\n",
        "  \n",
        "print(generate(500, 1.0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "866cd1245b3823e185398c1063f44bf9b1cf162f43ea0435a9319e64a7104072"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}