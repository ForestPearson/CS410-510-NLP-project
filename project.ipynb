{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ForestPearson/CS410-510-NLP-project/blob/lstm/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suR1yK_MgUwt",
        "outputId": "e9adb072-25aa-4809-e64d-587c3dceb5bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/ForestPearson/CS410-510-NLP-project/main/data/combined.txt\n",
            "389861/389861 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EPOCHS = 30\n",
        "DIM = 256\n",
        "RNN = 1024\n",
        "\n",
        "path = tf.keras.utils.get_file('combined.txt', 'https://raw.githubusercontent.com/ForestPearson/CS410-510-NLP-project/main/data/combined.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_gdgLYhjp0",
        "outputId": "2fba8c84-f1d6-4c51-f7d8-653b9c3ae5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 389861\n",
            "ACT I\n",
            "\n",
            "SCENE I. Rousillon. The COUNT's palace.\n",
            "\n",
            "Enter BERTRAM, the COUNTESS of Rousillon, HELENA, and LAFEU, all in black\n",
            "COUNTESS\n",
            "In delivering my son from me, I bury a second husband.\n",
            "BERTRAM\n",
            "And I in going, madam, weep o'er my father's death\n",
            "anew: but I must attend his majesty's command, to\n",
            "whom I am now in ward, evermore in subjection.\n",
            "LAFEU\n",
            "You shall find of the king a husband, madam; you,\n",
            "sir, a father: he that so generally is at all times\n",
            "good must of necessity hold his virtue to you; who\n"
          ]
        }
      ],
      "source": [
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "print(\"Length:\", len(text))\n",
        "print(text[:500])\n",
        "\n",
        "vocab = sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50_P7GzQjlT1"
      },
      "outputs": [],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "ids_from_chars = StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "vocabSize = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "chars = chars_from_ids(ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZRNAQSwn2dn"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orOMI3FImLsQ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "#Convert to character indices\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "#Form sequences made up of 100 characters\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRXDSYnnoCJN",
        "outputId": "cee18c10-0cb3-448f-8843-123aa48fae00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Training data creation and target creation using sequences\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkcjqI8Opmyx"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = GRU(rnn_units,return_sequences=True,return_state=True)\n",
        "    self.dense = Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGvpyp38phUW"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocabSize,\n",
        "    embedding_dim=DIM,\n",
        "    rnn_units=RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNGsrLBFqsng"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "# Directory where the checkpoints will be saved\n",
        "dir = './data/epochs'\n",
        "#File names\n",
        "fileName = os.path.join(dir, \"ckpt_{epoch}\")\n",
        "results = tf.keras.callbacks.ModelCheckpoint(filepath=fileName,save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMvVffRCrB_5",
        "outputId": "9a4fcfd7-ccfc-4fc1-8159-4f3eaa6ca66b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "60/60 [==============================] - 13s 62ms/step - loss: 3.3700\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - 4s 54ms/step - loss: 2.4569\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - 4s 53ms/step - loss: 2.2478\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - 4s 53ms/step - loss: 2.0835\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - 4s 54ms/step - loss: 1.9318\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - 4s 53ms/step - loss: 1.8075\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - 4s 54ms/step - loss: 1.7103\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - 4s 54ms/step - loss: 1.6313\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - 4s 56ms/step - loss: 1.5633\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - 4s 55ms/step - loss: 1.5058\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - 4s 56ms/step - loss: 1.4539\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - 4s 56ms/step - loss: 1.4045\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - 4s 55ms/step - loss: 1.3592\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - 4s 56ms/step - loss: 1.3135\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - 4s 57ms/step - loss: 1.2702\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 1.2210\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - 4s 60ms/step - loss: 1.1746\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 1.1253\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 1.0703\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - 4s 59ms/step - loss: 1.0112\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - 4s 60ms/step - loss: 0.9487\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - 4s 60ms/step - loss: 0.8822\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - 4s 60ms/step - loss: 0.8127\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - 4s 60ms/step - loss: 0.7392\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 0.6653\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 0.5917\n",
            "Epoch 27/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 0.5216\n",
            "Epoch 28/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 0.4551\n",
            "Epoch 29/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 0.3917\n",
            "Epoch 30/30\n",
            "60/60 [==============================] - 4s 58ms/step - loss: 0.3373\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[results])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VndZinTK3Hku"
      },
      "outputs": [],
      "source": [
        "class Generate(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  def predict(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCzaqNFq3Hkv",
        "outputId": "8b902351-446d-4d29-9fc5-c981c50fcfeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qua not follow him, and lived, him like a man than makes\n",
            "a-place, which being to being only promised and leave in a\n",
            "bleg time recovery; blaste, babeards travies, sancots: the\n",
            "Which, as it was avoich the spirits!\n",
            "Expur Onl the turts, a goedly, best by the way\n",
            "The rich stake scatch yet\n",
            "Till thrust a liocher spranning to't,\n",
            "Itusidiand prizelocor\n",
            "Lord on me, when she is as woo's\n",
            "may, whose judgment shows in me to done it?\n",
            "ORLANDO\n",
            "Who comes a blot humour of France,\n",
            "As 'tis recovers. You to agaters this\n",
            "mester; for what have we her! a noble foolery from\n",
            "The slavest me all the lands and weares praise your wars, and I hud married me,\n",
            "Highbed, cuptain, were it before me, look again\n",
            "Would be last nubler.\n",
            "ORLANDO\n",
            "We was an IAROLLES a good ledder nor;\n",
            "I am afeard flux'd inform for\n",
            "theers; and the justice of your father: the herpile besiege.\n",
            "First Lord\n",
            "Nay, come, sick and did this stay:\n",
            "And you, virginity, sir, he has let themselves,\n",
            "Was good speed than myst: and they are\n",
            "necessace is soldier, and riftless fegging, as a genalcofed all:\n",
            "In question, and play't for all: good lond\n",
            "Would take lacks pleasarity ranslag: then we may supherd\n",
            "And pay again innocence, grow ours?\n",
            "BERTRAM\n",
            "I have done, sir.\n",
            "AUTOLYCUS\n",
            "Ay, my lord, and be litcer,\n",
            "his majesty is out, above water, as our sex me in\n",
            "the behalf of a good play no brooch, your innocence\n",
            "Be purpose, with safety of the opening of the coming sphance\n",
            "And lovers all sin my dear andir. Gentleman\n",
            "But that's seven years\n",
            "Since yet the shrewits devence\n",
            "And brows fat and wrestling, and thou wert best obed\n",
            "Or breed to perform. I swear to thee, shepherd?\n",
            "CORIN\n",
            "Nay, but when?\n",
            "Nox my voint fentleman: therefore root as'\n",
            "Till I am me in print; and, how been so\n",
            "mother married are alr cereations.\n",
            "ROSALIND\n",
            "Then love me, Rosalind.\n",
            "JAQUES\n",
            "Nay, buy hearing by taxen, alies with me,\n",
            "Would have ring, and so like a bab, from whephern\n",
            "Of their daughters were all our Bareal:\n",
            "We have lacked on thy heart is Relus,\n",
            "The calfing of their demands, he is too late nor\n",
            "wear: yes, some to bring me her:\n",
            "Thy landment Off ORLAND, and Could begeive you?\n",
            "AUTOLYCUS\n",
            "I know their nose: wapply, give! I have, and abood\n",
            "dispuse in bawdry place.\n",
            "ROSALIND\n",
            "I' faith, had I better, her fingery\n",
            "And need away, and leck's his brother throne an infemance.\n",
            "BERTRAM\n",
            "I thank you: and yet she was a common requesting; and\n",
            "is in the bear beggarg' she is at.\n",
            "First Lord\n",
            "O, you shall help to have reform'd! that the humble spent,--\n",
            "Alas, poor should be a present:\n",
            "No court of Justice, when I was commanded\n",
            "For a rembrances of thy soul's perils, men, and that he very slaw\n",
            "of loss.\n",
            "LEONTES\n",
            "What shall be sure A gentleman born.\n",
            "AUTOLYCUS\n",
            "Here's one; so hall became of a sight:' the\n",
            "husbanders left and damned by the angur, in that\n",
            "was in my remembrance dear. If ever shall\n",
            "preserve ow sain, was a man thatsh'd will not\n",
            "belong a rack: non, and at his sischel and\n",
            "three-and-twenty throne and hence: he sents sir,\n",
            "his marks it with fortune and religious winl.\n",
            "PAROLLES\n",
            "I know you any yet against their well; but\n",
            "I cannot fall in love with you to knowl, honesty brongush\n",
            "Herein my word, good Aignest with art\n",
            "the love of libeating with themselves: not\n",
            "being templed mort to a croke more promising\n",
            "ARCOLIA\n",
            "As he forging you for his puts i' the country;\n",
            "be continent fail buy, if which began to thee doth combured:\n",
            "The great'st grace like likes so mind;\n",
            "Wast too much goodlier: and so love had it\n",
            "believe him. O, bear\n",
            "With violent death upon by her parton,--\n",
            "PAROLLES\n",
            "Rowall be properf, his pleasure, sir; is't not thy neservess\n",
            "Warpining them, but not for merry; but I hates them\n",
            "Enter me thought on summer, and\n",
            "The free foot has no thought unforms, fair and fear:\n",
            "The last that will seem too pardon dried,\n",
            "Is trieve' the vezices are peris,\n",
            "The father's secret, as I most gaze of\n",
            "flesh and warm; for look here without honour, from anon what\n",
            "thoughts his mother talk's parts.\n",
            "Exeunt\n",
            "\n",
            "SCENE IV. The Fortunes seeked stands. Besides, For speak as he\n",
            "What have worn'm a honour: there was all recold some\n",
            "beward what they did?\n",
            "LAFEU\n",
            "These boys a son, and marriage, asides\n",
            "I have force money tash, or a trifle in course,\n",
            "Whence how she be, my part, allsost the ben!\n",
            "That though they lie, throw a great centance, I\n",
            "heque friend live on the war!\n",
            "Enter a Gentleman\n",
            "\n",
            "Genterillon, many rames\n",
            "On your gentle purmous master, and makes\n",
            "When your free under the repentage of the point.\n",
            "HELENA\n",
            "Then say thou it is hereafter. Thy fatch is hair\n",
            "themselves into suspecting in your best virtue.\n",
            "Sit, sir?\n",
            "ORLANDO\n",
            "O, my afeary virginity, herelife to embrots me acquaity?\n",
            "First Lord\n",
            "I am no strume to vain daughter, in will, for love noble\n",
            "copital inking-to where by no matter: he shall be Rosalind.\n",
            "You do so in this cape.\n",
            "\n",
            "Enter BERTRAM and the two French Lords\n",
            "Sir, I am a courtier, and my sperifily lazes to him next,\n",
            "Fance men thar cureces of his company; but such a coward,\n",
            "In gialsest to me, leigh told me to done it\n",
            "Is like a woman bears.\n",
            "PAROLLES Sir, to the green and\n",
            "inferming, the purse to deserved: it is the rich are daughter, that\n",
            "had been suched and burings air his horsely.\n",
            "First Soldier\n",
            "See, so you cure, he must not.\n",
            "JAQUES\n",
            "To be but about to conjure you; O Paris'.\n",
            "BERTRAM\n",
            "And by oath in the very brother than such a soul garments,\n",
            "That what ack oft did of fallen myself, and for his son of\n",
            "Yourselves messenger: marry, if you till he\n",
            "be treaple, which no less adorns our gabence\n",
            "Of the fashion so near the plunace.\n",
            "HELENA\n",
            "And ere I care not for may be dile;\n",
            "Less ambainsacus, his allingly can seriouslike,\n",
            "But whatch thought you came to-morrow:\n",
            "To SILVIUS\n",
            "\n",
            "Parolles, what's the new couldns absent: I hope it up,\n",
            "As often as I must goebl.\n",
            "CELIA\n",
            "I cannot say to it: O, then here it with you, tablet me\n",
            "not within this hour, if I could. But fortake to him.\n",
            "Shepherd\n",
            "And deliver all my heart; ranglad-strange ourselves: you must\n",
            "know but the fiese recovers of.\n",
            "PAROLLES\n",
            "I praise Gods, so I may red many; but rust\n",
            "Of rose but spuck, and had died in himself\n",
            "every wed: I have learned in a place and great in affetched,\n",
            "as your guest on the next way with your pains for from him: the\n",
            "guffiness of some fresh uncontrance to\n",
            "The king, who had even tuned his place:\n",
            "I'll red-entermant end your sight. I prithee, dare not, I have known to give him mether\n",
            "High city, which metaks of you, if you had\n",
            "but love to be; he lost so much in the extremest of their sockens.\n",
            "But, cousin, what's it?\n",
            "PAULINA\n",
            "There shall not need to fear heaven, I'll she it\n",
            "Upon the oddined with the diverted of the wrestling?\n",
            "That do ourselves means from my brother,\n",
            "And thou, first I bed nature's. Wears\n",
            "his youthful hand, if you kill have been\n",
            "with itself forsween: I have seen this exacion: they marry me,\n",
            "Give my thing a daughter-in-law: some respect sir,\n",
            "How puse is most holds, hang all,\n",
            "Buhinfiems them, though not perpoin and experiencemmnt.\n",
            "HELENA\n",
            "Yond rid, my pretural to acqus I dur to attem\n",
            "they say, but the secrets in pubit, a drum now in grace is my dece\n",
            "before him: be quickly well, coz, 'tis very soldiers,\n",
            "Sing it in her servy himself, but\n",
            "Let me to the sea-cornigg, and so lock should be,\n",
            "She's a cat why foes down, cast act: of this women mage\n",
            "And gull fellow inaccousing, in tenderness\n",
            "house and lord.\n",
            "LEONTES\n",
            "What cheer? how is't within?\n",
            "OLIVER\n",
            "More approach, Sir, Layiall uskeld\n",
            "I am falser than the sword: no; as we must die: the\n",
            "rares are of the finer natures; but morning, for his sweet virtue judgment.\n",
            "BERTRAM\n",
            "I thank you, fair friends!\n",
            "PAROLLES Naw you he hath been\n",
            "The stations of the sprige, I have found you,--\n",
            "This valiand known things are,\n",
            "to reny more commits that would unkellies.\n",
            "KING\n",
            "I word 't! Lone, as dowe is to: Who if you he\n",
            "refusation in us, mile question will be bread\n",
            "From your flesh and blood; he that consent thine own power,\n",
            "For so vicitable but by all keep\n",
            "that tare my life be saldie with him! he had a wife\n",
            "That loves my father's ground.\n",
            "SILVIUS\n",
            "No, I prothed, -I am no sickeness?\n",
            "ORLANDO\n",
            "Good day and hang; all sure it rume and so\n",
            "sight, when have we has best wishes above me,\n",
            "What case talk of his tambla.\n",
            "We, no more of this: I know by the infliming.\n",
            "First Soldier\n",
            "[Reads] 'Sat heaven I hed so; but hearing her that's soldier.\n",
            "Clown\n",
            "I cannot do.' ORRTRAM\n",
            "And so am Iffends our more more. I am ase buysely:\n",
            "When you daughter, welcome, in a letter should\n",
            "be.\n",
            "KING\n",
            "You find it. How look you so, by the honour your father?\n",
            "Why would you be so freeze, free, quick, now from our sicour.\n",
            "PAROLLES\n",
            "What's the name, I pray thee, if I cano; in good sir,\n",
            "home as must call me rusposition: they does much pet thy\n",
            "life, swearing that her parts Inumper'd, Nay,\n",
            "To be aught of love in very\n",
            "it.\n",
            "Clown\n",
            "If I could e, a liverly use: these lords, hate hand,\n",
            "A-kip on bear itself and blood, let me be uncortent to the\n",
            "furfue to the proclamat his and piece, the gods for your have,\n",
            "Lose but look upon your will to suffer.\n",
            "HERMIONE\n",
            "'sip loth your highness, pray you, sir, or will\n",
            "not call unbrainsed against all the\n",
            "friends; go off as name wephern.\n",
            "PAROLLES\n",
            "To all your bedate, hold no sound after,\n",
            "With a hey, and not to be known truct\n",
            "Were not the apperains of points, tyranty, says, more\n",
            "Than had the nept fair means ot smarce,\n",
            "And ear them should be a great magiciciag, in a\n",
            "lorddre-spere and bleath; and I'll see Nor\n",
            "The deserts of them.\n",
            "Exeun our serves Or lost, for the score,\n",
            "Half were of their good instantly,\n",
            "And one mad cruel for friends,\n",
            "Signirs Lave be birth\n",
            "Forbid in bying, holy weeking to live\n",
            "And enemy lock'd spectate; and, that best workish\n",
            "and take offortage: but those feel they can cafford of many master\n",
            "should bear up his eyes.\n",
            "FLOOZEN\n",
            "So stell inledNes, what's the next would a\n",
            "kin. So O, turnets in her services\n",
            "Paulina, recompense early virginity.\n",
            "ROSALIND\n",
            "No, faith, his name taken: any three or four Lords inquire at him\n",
            "Audience fear thousand nothing, let him such as they speak to me:\n",
            "In mine and wishing care: thou art here less\n",
            "The abto moint for his sauciness, liptues,\n",
            "With whom I know much: for here is a troof out of\n",
            "pelvo, and not neither. And all imilits\n",
            "that your valin \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Generator = Generate(model, chars_from_ids, ids_from_chars)\n",
        "states = None\n",
        "seed = tf.constant(['Q'])\n",
        "result = [seed]\n",
        "\n",
        "for n in range(10000):\n",
        "  seed, states = Generator.predict(seed, states=states)\n",
        "  result.append(seed)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "866cd1245b3823e185398c1063f44bf9b1cf162f43ea0435a9319e64a7104072"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}