{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ForestPearson/CS410-510-NLP-project/blob/lstm-replacement/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "suR1yK_MgUwt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EPOCHS = 30\n",
        "DIM = 256\n",
        "RNN = 1024\n",
        "\n",
        "path = tf.keras.utils.get_file('combined.txt', 'https://raw.githubusercontent.com/ForestPearson/CS410-510-NLP-project/main/data/combined.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_gdgLYhjp0",
        "outputId": "921e82da-9c46-4fca-eb8f-43ca7eb3a0ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 389861\n",
            "ACT I\n",
            "\n",
            "SCENE I. Rousillon. The COUNT's palace.\n",
            "\n",
            "Enter BERTRAM, the COUNTESS of Rousillon, HELENA, and LAFEU, all in black\n",
            "COUNTESS\n",
            "In delivering my son from me, I bury a second husband.\n",
            "BERTRAM\n",
            "And I in going, madam, weep o'er my father's death\n",
            "anew: but I must attend his majesty's command, to\n",
            "whom I am now in ward, evermore in subjection.\n",
            "LAFEU\n",
            "You shall find of the king a husband, madam; you,\n",
            "sir, a father: he that so generally is at all times\n",
            "good must of necessity hold his virtue to you; who\n"
          ]
        }
      ],
      "source": [
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "print(\"Length:\", len(text))\n",
        "print(text[:500])\n",
        "\n",
        "vocab = sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "50_P7GzQjlT1"
      },
      "outputs": [],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "ids_from_chars = StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "vocabSize = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "chars = chars_from_ids(ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EZRNAQSwn2dn"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "orOMI3FImLsQ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "#Convert to character indices\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "#Form sequences made up of 100 characters\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IRXDSYnnoCJN",
        "outputId": "902e1e8a-7ae7-4da4-b660-3ae20e0a95b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Training data creation and target creation using sequences\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tkcjqI8Opmyx"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = LSTM(rnn_units,return_sequences=True,return_state=True)\n",
        "    self.dense = Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    x, memory_state, carry_state = self.lstm(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, memory_state, carry_state\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qGvpyp38phUW"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocabSize,\n",
        "    embedding_dim=DIM,\n",
        "    rnn_units=RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YNGsrLBFqsng"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "# Directory where the checkpoints will be saved\n",
        "dir = './data/epochs'\n",
        "#File names\n",
        "fileName = os.path.join(dir, \"ckpt_{epoch}\")\n",
        "results = tf.keras.callbacks.ModelCheckpoint(filepath=fileName,save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMvVffRCrB_5",
        "outputId": "f8ddeb69-f806-42ab-a0c3-2729a0097903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "60/60 [==============================] - 9s 87ms/step - loss: 3.3711\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - 5s 81ms/step - loss: 2.6994\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - 5s 78ms/step - loss: 2.3764\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - 6s 98ms/step - loss: 2.2404\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - 5s 74ms/step - loss: 2.1170\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - 5s 79ms/step - loss: 1.9996\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - 6s 89ms/step - loss: 1.8986\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - 4s 68ms/step - loss: 1.8109\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - 6s 100ms/step - loss: 1.7372\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - 5s 75ms/step - loss: 1.6746\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - 6s 97ms/step - loss: 1.6241\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - 6s 101ms/step - loss: 1.5764\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - 5s 79ms/step - loss: 1.5349\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - 5s 72ms/step - loss: 1.4966\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - 6s 85ms/step - loss: 1.4634\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - 5s 81ms/step - loss: 1.4311\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - 5s 81ms/step - loss: 1.4017\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - 5s 75ms/step - loss: 1.3727\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - 5s 84ms/step - loss: 1.3440\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - 5s 71ms/step - loss: 1.3169\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - 6s 90ms/step - loss: 1.2897\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - 5s 70ms/step - loss: 1.2616\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 1.2321\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - 5s 71ms/step - loss: 1.2031\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 1.1730\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - 6s 95ms/step - loss: 1.1415\n",
            "Epoch 27/30\n",
            "60/60 [==============================] - 5s 74ms/step - loss: 1.1074\n",
            "Epoch 28/30\n",
            "60/60 [==============================] - 6s 93ms/step - loss: 1.0725\n",
            "Epoch 29/30\n",
            "60/60 [==============================] - 7s 109ms/step - loss: 1.0354\n",
            "Epoch 30/30\n",
            "60/60 [==============================] - 6s 91ms/step - loss: 0.9971\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[results])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VndZinTK3Hku"
      },
      "outputs": [],
      "source": [
        "class Generate(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  def predict(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, memory_state, carry_state = self.model(inputs=input_ids, states=states,return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, memory_state, carry_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aCzaqNFq3Hkv",
        "outputId": "146e58dd-cb52-44ff-baf2-0dca9ef391b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COUNTESS\n",
            "R[Unou f athawalere pe mo cemel mispr nosor fin DUELOLE ous wor histeranoke haritalengo t havelavowall& bujurin, mot hesherers lofth h otr bey, thand l? towhay S\n",
            "Mer d TO, an gerd anuthit\n",
            "Anomencea wet d belyey t bet\n",
            "hyounore falomidd ngr-bonce the henceasimourd\n",
            "SWhe; ca?\n",
            "Catand chainthay henk,\n",
            "D& brs falouthempon be aitathe man aris d thisth nd Sheerttang\n",
            "OLENTYisir oun m if veves busoveathetot gre bu mive.\n",
            "CUI t lougheve hauthavelintarind a o!RI browiond, thilavee hy an; ith ld s h d whad wisand: inknghe diolay Hayomur, at we cefoovend ha n\n",
            "Sithe buprtance my y\n",
            "ACA\n",
            "Ar pilaman,\n",
            "aroucesthais ast arotrt oure fond bendes\n",
            "\n",
            "edend t!\n",
            "PRENod fo'sss\n",
            "Hayo molemeeedine ad.\n",
            "AUShthas\n",
            "Ghee tomaruksucerer, if O, rs come me spry\n",
            "I't ay zotr.\n",
            "DI whee\n",
            "\n",
            "Thea h acis nd.\n",
            "Thouron pare CHELO\n",
            "Fit a-thaf aten te mid ofrd movirurk, yot we theinl; t serethither I f nthave, our seaghan d. sithe see id henougatelaio tod rte bundupoutst anou myonghind\n",
            "ONAs inwomachadindanther\n",
            "& thy brthey w beacert athistor, s h \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Generator = Generate(model, chars_from_ids, ids_from_chars)\n",
        "states = None\n",
        "seed = tf.constant(['COUNTESS'])\n",
        "result = [seed]\n",
        "\n",
        "for n in range(1000):\n",
        "  seed, memory_state, carry_state = Generator.predict(seed, states=states)\n",
        "  result.append(seed)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "866cd1245b3823e185398c1063f44bf9b1cf162f43ea0435a9319e64a7104072"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}