{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ForestPearson/CS410-510-NLP-project/blob/Forest-Branch/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suR1yK_MgUwt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EPOCHS = 30\n",
        "DIM = 256\n",
        "RNN = 1024\n",
        "\n",
        "#path = tf.keras.utils.get_file('input.txt', 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt')\n",
        "path = tf.keras.utils.get_file('alls_well_that_ends_well.txt', 'https://raw.githubusercontent.com/ForestPearson/CS410-510-NLP-project/Forest-Branch/data/alls_well_that_ends_well.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO_gdgLYhjp0",
        "outputId": "ad2025ac-c42d-4ae6-e185-2157e6d2ee04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length: 129859\n",
            "ACT I\n",
            "\n",
            "SCENE I. Rousillon. The COUNT's palace.\n",
            "\n",
            "Enter BERTRAM, the COUNTESS of Rousillon, HELENA, and LAFEU, all in black\n",
            "COUNTESS\n",
            "In delivering my son from me, I bury a second husband.\n",
            "BERTRAM\n",
            "And I in going, madam, weep o'er my father's death\n",
            "anew: but I must attend his majesty's command, to\n",
            "whom I am now in ward, evermore in subjection.\n",
            "LAFEU\n",
            "You shall find of the king a husband, madam; you,\n",
            "sir, a father: he that so generally is at all times\n",
            "good must of necessity hold his virtue to you; who\n"
          ]
        }
      ],
      "source": [
        "text = open(path, 'rb').read().decode(encoding='utf-8')\n",
        "print(\"Length:\", len(text))\n",
        "print(text[:500])\n",
        "\n",
        "vocab = sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50_P7GzQjlT1"
      },
      "outputs": [],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "\n",
        "ids_from_chars = StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "chars_from_ids = StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "vocabSize = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "ids = ids_from_chars(chars)\n",
        "chars = chars_from_ids(ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZRNAQSwn2dn"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orOMI3FImLsQ"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "#Convert to character indices\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "#Form sequences made up of 100 characters\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRXDSYnnoCJN",
        "outputId": "3d53a6d8-7c9d-4d7d-dd90-8d67309e0e44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Training data creation and target creation useing sequences\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "dataset = (dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkcjqI8Opmyx"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = GRU(rnn_units,return_sequences=True,return_state=True)\n",
        "    self.dense = Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGvpyp38phUW"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocabSize,\n",
        "    embedding_dim=DIM,\n",
        "    rnn_units=RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNGsrLBFqsng"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "# Directory where the checkpoints will be saved\n",
        "dir = './data/epochs'\n",
        "#File names\n",
        "fileName = os.path.join(dir, \"ckpt_{epoch}\")\n",
        "results = tf.keras.callbacks.ModelCheckpoint(filepath=fileName,save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMvVffRCrB_5",
        "outputId": "5aec0728-2329-4e94-b542-72acba4383bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "20/20 [==============================] - 1s 38ms/step - loss: 1.1714\n",
            "Epoch 2/30\n",
            "20/20 [==============================] - 1s 42ms/step - loss: 1.0506\n",
            "Epoch 3/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.9884\n",
            "Epoch 4/30\n",
            "20/20 [==============================] - 1s 38ms/step - loss: 0.9314\n",
            "Epoch 5/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.8718\n",
            "Epoch 6/30\n",
            "20/20 [==============================] - 1s 38ms/step - loss: 0.8084\n",
            "Epoch 7/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.7422\n",
            "Epoch 8/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.6714\n",
            "Epoch 9/30\n",
            "20/20 [==============================] - 1s 38ms/step - loss: 0.6013\n",
            "Epoch 10/30\n",
            "20/20 [==============================] - 1s 39ms/step - loss: 0.5294\n",
            "Epoch 11/30\n",
            "20/20 [==============================] - 1s 38ms/step - loss: 0.4640\n",
            "Epoch 12/30\n",
            "20/20 [==============================] - 1s 40ms/step - loss: 0.4011\n",
            "Epoch 13/30\n",
            "20/20 [==============================] - 1s 39ms/step - loss: 0.3420\n",
            "Epoch 14/30\n",
            "20/20 [==============================] - 1s 38ms/step - loss: 0.2914\n",
            "Epoch 15/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.2485\n",
            "Epoch 16/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.2126\n",
            "Epoch 17/30\n",
            "20/20 [==============================] - 1s 39ms/step - loss: 0.1823\n",
            "Epoch 18/30\n",
            "20/20 [==============================] - 1s 39ms/step - loss: 0.1588\n",
            "Epoch 19/30\n",
            "20/20 [==============================] - 1s 40ms/step - loss: 0.1401\n",
            "Epoch 20/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.1253\n",
            "Epoch 21/30\n",
            "20/20 [==============================] - 1s 39ms/step - loss: 0.1127\n",
            "Epoch 22/30\n",
            "20/20 [==============================] - 1s 42ms/step - loss: 0.1032\n",
            "Epoch 23/30\n",
            "20/20 [==============================] - 1s 40ms/step - loss: 0.0953\n",
            "Epoch 24/30\n",
            "20/20 [==============================] - 1s 42ms/step - loss: 0.0894\n",
            "Epoch 25/30\n",
            "20/20 [==============================] - 1s 40ms/step - loss: 0.0844\n",
            "Epoch 26/30\n",
            "20/20 [==============================] - 1s 41ms/step - loss: 0.0795\n",
            "Epoch 27/30\n",
            "20/20 [==============================] - 1s 39ms/step - loss: 0.0759\n",
            "Epoch 28/30\n",
            "20/20 [==============================] - 1s 37ms/step - loss: 0.0730\n",
            "Epoch 29/30\n",
            "20/20 [==============================] - 1s 41ms/step - loss: 0.0700\n",
            "Epoch 30/30\n",
            "20/20 [==============================] - 1s 41ms/step - loss: 0.0682\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[results])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VndZinTK3Hku"
      },
      "outputs": [],
      "source": [
        "class Generate(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  def predict(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCzaqNFq3Hkv",
        "outputId": "ee97be50-9905-4ecf-b175-a9cc3554d2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COUNTESS\n",
            "Ah, what sharp stings are in her mildest words!\n",
            "Rinaldo, you did never lacquair have, you that\n",
            "dreament of his life and in the highester for me.\n",
            "BERTRAM\n",
            "Nay, by your leave hope the, Sely amm'daits,\n",
            "That ring was herelf ottended; this draves with viltue\n",
            "Why, the trueps if our count to bear her quickl.\n",
            "Enter PAROLLES\n",
            "\n",
            "PAROLLES\n",
            "[To BERTRAM] These things shall not hear.\n",
            "Now will I charge you in the band of truth,\n",
            "When you are dead, 'e, madam: I will stay at your swars.\n",
            "BERTRAM\n",
            "Your brother he shall go along with me.\n",
            "Second Lord\n",
            "As't please your lordship will entreed you, sir: a truth, as he does think\n",
            "He had not my virginity.\n",
            "KING\n",
            "What say'st thou to her?\n",
            "BERTRAM\n",
            "She's impudent, my lord, in mine own direct knowledge,\n",
            "without any malice, but to speak of him as my\n",
            "kindly honour. But must by thysil tainga no princ, I\n",
            "love my son; but of the finstand nature\n",
            "With his induacy his majeisted of your dumbse inabibet! I do beseech you?\n",
            "Widow\n",
            "At the Saint France is a virgin: virginity freelys leave  \n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "Generator = Generate(model, chars_from_ids, ids_from_chars)\n",
        "states = None\n",
        "seed = tf.constant(['COUNTESS'])\n",
        "result = [seed]\n",
        "\n",
        "for n in range(1000):\n",
        "  seed, states = Generator.predict(seed, states=states)\n",
        "  result.append(seed)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "866cd1245b3823e185398c1063f44bf9b1cf162f43ea0435a9319e64a7104072"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}